import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque

# Helper function to get all rotations of an item
def get_rotations(item):
    rotations = [
        (item[0], item[1], item[2]),
        (item[0], item[2], item[1]),
        (item[1], item[0], item[2]),
        (item[1], item[2], item[0]),
        (item[2], item[0], item[1]),
        (item[2], item[1], item[0])
    ]
    return rotations

# Environment
class PackingEnvironment:
    def __init__(self, cuboid_dimensions, items):
        self.cuboid_dimensions = cuboid_dimensions
        self.items = items
        self.reset()

    def reset(self):
        self.cuboid = np.zeros(self.cuboid_dimensions)
        self.current_item_index = 0
        self.done = False
        return self._get_state()

    def _get_state(self):
        if self.done or self.current_item_index >= len(self.items):
            return np.zeros(np.prod(self.cuboid_dimensions) + 3)  # Updated to 3 for current item dimensions
        cuboid_flattened = self.cuboid.flatten()
        item_dims = np.array(self.items[self.current_item_index])
        state = np.concatenate((cuboid_flattened, item_dims))
        return state

    def step(self, position_rotation):
        if self.done:
            raise Exception("Environment is done. Reset to start again.")
        
        position, rotation = position_rotation
        item = self.items[self.current_item_index]
        rotated_item = get_rotations(item)[rotation]
        
        if self._can_place_item(position, rotated_item):
            self._place_item(position, rotated_item)
            reward = 1  # Reward for successfully placing an item
            self.current_item_index += 1
            if self.current_item_index >= len(self.items):
                self.done = True
        else:
            reward = -1  # Penalty for an invalid placement
        
        next_state = self._get_state()
        return next_state, reward, self.done

    def _can_place_item(self, position, item):
        x, y, z = position
        item_x, item_y, item_z = item
        if (x + item_x > self.cuboid_dimensions[0] or
            y + item_y > self.cuboid_dimensions[1] or
            z + item_z > self.cuboid_dimensions[2]):
            return False
        if np.any(self.cuboid[x:x+item_x, y:y+item_y, z:z+item_z]):
            return False
        return True

    def _place_item(self, position, item):
        x, y, z = position
        item_x, item_y, item_z = item
        self.cuboid[x:x+item_x, y:y+item_y, z:z+item_z] = 1

# DQN Network
class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 256)
        self.fc3 = nn.Linear(256, output_dim * 6)  # 6 possible rotations

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

# Agent
class Agent:
    def __init__(self, model, env, lr=0.001, gamma=0.99, epsilon=0.1, batch_size=32, memory_size=1000):
        self.model = model
        self.env = env
        self.optimizer = optim.Adam(model.parameters(), lr=lr)
        self.gamma = gamma
        self.epsilon = epsilon
        self.batch_size = batch_size
        self.memory = deque(maxlen=memory_size)

    def select_action(self, state):
        if random.random() < self.epsilon:
            return (random.randint(0, np.prod(self.env.cuboid_dimensions) - 1), random.randint(0, 5))
        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
        q_values = self.model(state)
        return divmod(q_values.argmax().item(), 6)  # Adjust to get position and rotation

    def store_transition(self, transition):
        self.memory.append(transition)

    def train(self):
        if len(self.memory) < self.batch_size:
            return
        
        batch = random.sample(self.memory, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        states = torch.tensor(states, dtype=torch.float32)
        actions = torch.tensor(actions, dtype=torch.int64)  # Ensure actions are int64
        rewards = torch.tensor(rewards, dtype=torch.float32)
        next_states = torch.tensor(next_states, dtype=torch.float32)
        dones = torch.tensor(dones, dtype=torch.float32)

        # Split actions into position and rotation
        positions = actions[:, 0]  # First part is the position index
        rotations = actions[:, 1]   # Second part is the rotation index

        # Gather Q-values for the selected positions and rotations
        q_values = self.model(states).view(-1, 6)  # Reshape to (batch_size, output_dim * 6)
        q_values_selected = q_values.gather(1, rotations.unsqueeze(1)).squeeze()  # Gather based on rotation

        # Calculate next Q-values
        next_q_values = self.model(next_states).view(-1, 6)  # Ensure the shape is (batch_size, 6)
        max_next_q_values = next_q_values.max(1)[0]  # Get max Q-values for next states

        # Calculate target Q-values
        target_q_values = rewards + self.gamma * max_next_q_values * (1 - dones)

        # Compute loss
        loss = F.mse_loss(q_values_selected, target_q_values)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()


    # def train(self):
    #     if len(self.memory) < self.batch_size:
    #         return
        
    #     batch = random.sample(self.memory, self.batch_size)
    #     states, actions, rewards, next_states, dones = zip(*batch)

    #     states = torch.tensor(states, dtype=torch.float32)
    #     actions = torch.tensor(actions, dtype=torch.int64)  # Ensure actions are int64
    #     rewards = torch.tensor(rewards, dtype=torch.float32)
    #     next_states = torch.tensor(next_states, dtype=torch.float32)
    #     dones = torch.tensor(dones, dtype=torch.float32)

    #     # Fix the gather call to match the output dimensions
    #     q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze()
    #     next_q_values = self.model(next_states).max(1)[0]
    #     target_q_values = rewards + self.gamma * next_q_values * (1 - dones)

    #     loss = F.mse_loss(q_values, target_q_values)
    #     self.optimizer.zero_grad()
    #     loss.backward()
    #     self.optimizer.step()

# Example items and cuboid dimensions
cuboid_dimensions = (10, 10, 10)
items = [(2, 2, 2), (3, 2, 1), (1, 1, 1)]  # Variable number of items

env = PackingEnvironment(cuboid_dimensions, items)

# Define the network dimensions
input_dim = np.prod(cuboid_dimensions) + 3  # Cuboid state + item dimensions
output_dim = np.prod(cuboid_dimensions)  # Possible positions
model = DQN(input_dim, output_dim)

# Initialize the agent
agent = Agent(model, env)

# Training loop
num_episodes = 1000

for episode in range(num_episodes):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action = agent.select_action(state)
        position = np.unravel_index(action[0], env.cuboid_dimensions)
        next_state, reward, done = env.step((position, action[1]))
        agent.store_transition((state, action, reward, next_state, done))
        agent.train()
        state = next_state
        total_reward += reward

    print(f"Episode {episode+1}: Total Reward: {total_reward}")

print("Training finished.")

# Save the model
torch.save(model.state_dict(), 'dqn_model.pth')

# Load the model (Optional)
model = DQN(input_dim, output_dim)
model.load_state_dict(torch.load('dqn_model.pth'))
model.eval()

# Run inference
def run_inference(env, model):
    state = env.reset()
    total_reward = 0
    done = False
    positions_rotations = []

    while not done:
        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
        q_values = model(state_tensor)
        action = q_values.argmax().item()
        position_rotation = divmod(action, 6)
        position = np.unravel_index(position_rotation[0], env.cuboid_dimensions)
        positions_rotations.append((position, position_rotation[1]))
        next_state, reward, done = env.step((position, position_rotation[1]))
        state = next_state
        total_reward += reward

    return total_reward, positions_rotations

# Test with a new set of items and cuboid dimensions
new_items = [(2, 3, 1), (4, 2, 2), (1, 2, 1), (3, 3, 1)]
new_cuboid_dimensions = (10, 10, 10)
new_env = PackingEnvironment(new_cuboid_dimensions, new_items)

inference_reward, positions_rotations = run_inference(new_env, model)
print(f"Inference Total Reward: {inference_reward}")
print(f"Positions and Rotations: {positions_rotations}")
